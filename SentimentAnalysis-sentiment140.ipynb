{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74edab36",
   "metadata": {},
   "source": [
    "# The impact of dimensionality reduction in sentiment analysis classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb2b79",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1b9b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Time library\n",
    "import time\n",
    "\n",
    "# Dataset manipulation/vectorization libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Dimensionality Reduction Algorithms\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Supervised classifiers.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc3403",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f056615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KNeighborsClassifier(n_neighbors=10),\n",
       " LogisticRegression(max_iter=300),\n",
       " DecisionTreeClassifier(),\n",
       " RandomForestClassifier(),\n",
       " SVC(C=1),\n",
       " MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "        KNeighborsClassifier(10),\n",
    "        LogisticRegression(max_iter=300),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        SVC(kernel='rbf', C = 1),\n",
    "        MLPClassifier(activation='relu', hidden_layer_sizes=(128, 16), random_state=42),\n",
    "]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea5931",
   "metadata": {},
   "source": [
    "## Dataset: Sentiment140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cc451a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source           1                             2         3  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 4                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets\\\\sentiment140.csv\", encoding='latin-1', header=None)\n",
    "\n",
    "df = df.rename(columns={5: \"text\", 0: \"source\"}, errors='raise')\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be627fa6",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3e0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaner_lemmas(text):\n",
    "    ''' Takes in raw unformatted text and strips punctuation, removes whitespace,\n",
    "    strips numbers, tokenizes and stems.\n",
    "    Returns string of processed text to be used into CountVectorizer\n",
    "    '''\n",
    "    # Lowercase and strip everything except words\n",
    "    cleaner = re.sub(r\"[^a-zA-Z ]+\", ' ', text.lower())\n",
    "    # Tokenize\n",
    "    cleaner = word_tokenize(cleaner)\n",
    "    clean = []\n",
    "    for w in cleaner:\n",
    "        # filter out stopwords\n",
    "        if w not in stopWords:\n",
    "            # filter out short words\n",
    "            if len(w)>2:\n",
    "                # lemmatizer \n",
    "                clean.append(lemmatizer.lemmatize(w))\n",
    "    return ' '.join(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4031d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning duration: 258.8762502670288\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "t0 = time.time()\n",
    "'''\n",
    "label = []\n",
    "for i in df['source']:\n",
    "    if df['source'] == 1:\n",
    "        label.append('positive')\n",
    "    else:\n",
    "        label.append('negative')\n",
    "#    label.append(dataset.target_names[i])\n",
    "\n",
    "df['label'] = label\n",
    "'''\n",
    "df['clean_text'] = df.text.apply(lambda x: textcleaner_lemmas(x))\n",
    "t1 = time.time()\n",
    "\n",
    "df.head()\n",
    "print(\"Text cleaning duration:\", t1 - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5046424",
   "metadata": {},
   "source": [
    "# Text vectorization - Training/Test Set construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c538b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectorization duration: 18.882500171661377\n",
      "Training set dimensionality:  (1120000, 455257)\n",
      "Test set dimensionality:  (480000, 455257)\n"
     ]
    }
   ],
   "source": [
    "X = df['clean_text']\n",
    "y = df['source']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify = y)\n",
    "\n",
    "t0 = time.time()\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "#print(\"== TF-IDF VOCAB X_TRAIN =============================================================\\n\")\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "#print(\"== TF-IDF VOCAB X_TEST  =============================================================\\n\")\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Text vectorization duration:\", t1 - t0)\n",
    "print(\"Training set dimensionality: \", X_train_vec.shape)\n",
    "print(\"Test set dimensionality: \", X_test_vec.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80d8ff",
   "metadata": {},
   "source": [
    "# Models on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "\t === CLASSIFIER: KNeighborsClassifier(n_neighbors=10) - TARGET SPACE: original\n",
      "\t === Model training: 0.12750005722045898 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65    240000\n",
      "           4       0.64      0.42      0.51    240000\n",
      "\n",
      "    accuracy                           0.59    480000\n",
      "   macro avg       0.60      0.59      0.58    480000\n",
      "weighted avg       0.60      0.59      0.58    480000\n",
      "\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "\t === CLASSIFIER: LogisticRegression(max_iter=300) - TARGET SPACE: original\n",
      "\t === Model training: 59.36199975013733 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78    240000\n",
      "           4       0.77      0.80      0.78    240000\n",
      "\n",
      "    accuracy                           0.78    480000\n",
      "   macro avg       0.78      0.78      0.78    480000\n",
      "weighted avg       0.78      0.78      0.78    480000\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: DecisionTreeClassifier() - TARGET SPACE: original\n",
      "\t === Model training: 6551.956000089645 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71    240000\n",
      "           4       0.72      0.70      0.71    240000\n",
      "\n",
      "    accuracy                           0.71    480000\n",
      "   macro avg       0.71      0.71      0.71    480000\n",
      "weighted avg       0.71      0.71      0.71    480000\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: RandomForestClassifier() - TARGET SPACE: original\n",
      "\t === Model training: 65864.09399986267 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78    240000\n",
      "           4       0.78      0.77      0.77    240000\n",
      "\n",
      "    accuracy                           0.77    480000\n",
      "   macro avg       0.77      0.77      0.77    480000\n",
      "weighted avg       0.77      0.77      0.77    480000\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: SVC(C=1) - TARGET SPACE: original\n",
      "\t === Model training: 220003.91024947166 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79    240000\n",
      "           4       0.78      0.81      0.80    240000\n",
      "\n",
      "    accuracy                           0.79    480000\n",
      "   macro avg       0.79      0.79      0.79    480000\n",
      "weighted avg       0.79      0.79      0.79    480000\n",
      "\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for clf in models:\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"=================================================================================================\")\n",
    "    print(\"\\t === CLASSIFIER:\", clf, \"- TARGET SPACE: original\")\n",
    "    print(\"\\t === Model training:\", t1 - t0, \"sec\")\n",
    "\n",
    "    y_predicted = clf.predict(X_test_vec)\n",
    "\n",
    "    print(\"\\t === Classification Report\")\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"=================================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806daa5",
   "metadata": {},
   "source": [
    "# Models on the reduced dimensional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c025be",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_spaces = [ 10, 100, 1000, 10000 ]\n",
    "for target_space in reduced_spaces:\n",
    "    print(\"========== Working with target space of\", target_space, \"dimensions ============\")\n",
    "    SVD = TruncatedSVD(n_components=target_space, random_state=42)\n",
    "    X_train_red = SVD.fit_transform(X_train_vec)\n",
    "    X_test_red = SVD.transform(X_test_vec)\n",
    "\n",
    "    print(\"\\tInput space dimensionality (training):\", X_train_red.shape, X_train_vec.shape)\n",
    "    print(\"\\tInput space dimensionality (testing):\", X_test_red.shape, X_test_vec.shape)\n",
    "\n",
    "    for clf in models:\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train_red, y_train)\n",
    "        t1 = time.time()\n",
    "        print(\"=================================================================================================\")\n",
    "        print(\"\\t === CLASSIFIER:\", clf, \"- TARGET SPACE:\", target_space)\n",
    "        print(\"\\t === Model training:\", t1 - t0, \"sec\")\n",
    "\n",
    "        y_predicted = clf.predict(X_test_red)\n",
    "\n",
    "        print(\"\\t === Classification Report\")\n",
    "        print(\"\\t\", classification_report(y_test, y_predicted))\n",
    "        print(\"=================================================================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
