{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74edab36",
   "metadata": {},
   "source": [
    "# The impact of dimensionality reduction in sentiment analysis classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb2b79",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1b9b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Time library\n",
    "import time\n",
    "\n",
    "# Dataset manipulation/vectorization libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Dimensionality Reduction Algorithms\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Supervised classifiers.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc3403",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f056615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KNeighborsClassifier(n_neighbors=10),\n",
       " LogisticRegression(max_iter=300),\n",
       " DecisionTreeClassifier(),\n",
       " RandomForestClassifier(),\n",
       " SVC(C=1),\n",
       " MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "        KNeighborsClassifier(10),\n",
    "        LogisticRegression(max_iter=300),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        SVC(kernel='rbf', C = 1),\n",
    "        MLPClassifier(activation='relu', hidden_layer_sizes=(128, 16), random_state=42),\n",
    "]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea5931",
   "metadata": {},
   "source": [
    "## Dataset: IMDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cc451a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570300767074181121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:33 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>570300248553349120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:12:29 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>570299953286942721</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dhepburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:11:19 -0800</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>570295459631263746</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YupitsTate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:53:27 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  source  airline_sentiment_confidence negativereason  \\\n",
       "0  570306133677760513       1                        1.0000            NaN   \n",
       "1  570301130888122368       2                        0.3486            NaN   \n",
       "2  570301083672813571       1                        0.6837            NaN   \n",
       "3  570301031407624196       0                        1.0000     Bad Flight   \n",
       "4  570300817074462722       0                        1.0000     Can't Tell   \n",
       "5  570300767074181121       0                        1.0000     Can't Tell   \n",
       "6  570300616901320704       2                        0.6745            NaN   \n",
       "7  570300248553349120       1                        0.6340            NaN   \n",
       "8  570299953286942721       2                        0.6559            NaN   \n",
       "9  570295459631263746       2                        1.0000            NaN   \n",
       "\n",
       "   negativereason_confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "5                     0.6842  Virgin America                    NaN   \n",
       "6                     0.0000  Virgin America                    NaN   \n",
       "7                        NaN  Virgin America                    NaN   \n",
       "8                        NaN  Virgin America                    NaN   \n",
       "9                        NaN  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "5    jnardino                 NaN              0   \n",
       "6  cjmcginnis                 NaN              0   \n",
       "7       pilot                 NaN              0   \n",
       "8    dhepburn                 NaN              0   \n",
       "9  YupitsTate                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...         NaN   \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...         NaN   \n",
       "7  @VirginAmerica Really missed a prime opportuni...         NaN   \n",
       "8    @virginamerica Well, I didn't…but NOW I DO! :-D         NaN   \n",
       "9  @VirginAmerica it was amazing, and arrived an ...         NaN   \n",
       "\n",
       "               tweet_created    tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800               NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800               NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800         Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800               NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800               NaN  Pacific Time (US & Canada)  \n",
       "5  2015-02-24 11:14:33 -0800               NaN  Pacific Time (US & Canada)  \n",
       "6  2015-02-24 11:13:57 -0800  San Francisco CA  Pacific Time (US & Canada)  \n",
       "7  2015-02-24 11:12:29 -0800       Los Angeles  Pacific Time (US & Canada)  \n",
       "8  2015-02-24 11:11:19 -0800         San Diego  Pacific Time (US & Canada)  \n",
       "9  2015-02-24 10:53:27 -0800       Los Angeles  Eastern Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv(\"datasets\\\\tweets.csv\", encoding = \"utf-8\")\n",
    "\n",
    "df = df.rename(columns={\"text\": \"text\", \"airline_sentiment\": \"source\"}, errors='raise')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['source'] = le.fit_transform(df['source'])\n",
    "\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be627fa6",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3e0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaner_lemmas(text):\n",
    "    ''' Takes in raw unformatted text and strips punctuation, removes whitespace,\n",
    "    strips numbers, tokenizes and stems.\n",
    "    Returns string of processed text to be used into CountVectorizer\n",
    "    '''\n",
    "    # Lowercase and strip everything except words\n",
    "    cleaner = re.sub(r\"[^a-zA-Z ]+\", ' ', text.lower())\n",
    "    # Tokenize\n",
    "    cleaner = word_tokenize(cleaner)\n",
    "    clean = []\n",
    "    for w in cleaner:\n",
    "        # filter out stopwords\n",
    "        if w not in stopWords:\n",
    "            # filter out short words\n",
    "            if len(w)>2:\n",
    "                # lemmatizer \n",
    "                clean.append(lemmatizer.lemmatize(w))\n",
    "    return ' '.join(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4031d020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning duration: 3.3940000534057617\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>virginamerica plus added commercial experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>virginamerica today must mean need take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  source  airline_sentiment_confidence negativereason  \\\n",
       "0  570306133677760513       1                        1.0000            NaN   \n",
       "1  570301130888122368       2                        0.3486            NaN   \n",
       "2  570301083672813571       1                        0.6837            NaN   \n",
       "3  570301031407624196       0                        1.0000     Bad Flight   \n",
       "4  570300817074462722       0                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason_confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "                                          clean_text  \n",
       "0                        virginamerica dhepburn said  \n",
       "1  virginamerica plus added commercial experience...  \n",
       "2  virginamerica today must mean need take anothe...  \n",
       "3  virginamerica really aggressive blast obnoxiou...  \n",
       "4                 virginamerica really big bad thing  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "t0 = time.time()\n",
    "'''\n",
    "label = []\n",
    "for i in df['source']:\n",
    "    if df['source'] == 1:\n",
    "        label.append('positive')\n",
    "    else:\n",
    "        label.append('negative')\n",
    "#    label.append(dataset.target_names[i])\n",
    "\n",
    "df['label'] = label\n",
    "'''\n",
    "df['clean_text'] = df.text.apply(lambda x: textcleaner_lemmas(x))\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Text cleaning duration:\", t1 - t0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5046424",
   "metadata": {},
   "source": [
    "# Text vectorization - Training/Test Set construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c538b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectorization duration: 0.15700006484985352\n",
      "Training set dimensionality:  (10248, 9849)\n",
      "Test set dimensionality:  (4392, 9849)\n"
     ]
    }
   ],
   "source": [
    "X = df['clean_text']\n",
    "y = df['source']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify = y)\n",
    "\n",
    "t0 = time.time()\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "#print(\"== TF-IDF VOCAB X_TRAIN =============================================================\\n\")\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "#print(\"== TF-IDF VOCAB X_TEST  =============================================================\\n\")\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Text vectorization duration:\", t1 - t0)\n",
    "print(\"Training set dimensionality: \", X_train_vec.shape)\n",
    "print(\"Test set dimensionality: \", X_test_vec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80d8ff",
   "metadata": {},
   "source": [
    "# Models on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b0c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "\t === CLASSIFIER: KNeighborsClassifier(n_neighbors=10) - TARGET SPACE: original\n",
      "\t === Model training: 0.0009999275207519531 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      2753\n",
      "           1       0.49      0.42      0.45       930\n",
      "           2       0.66      0.56      0.60       709\n",
      "\n",
      "    accuracy                           0.72      4392\n",
      "   macro avg       0.65      0.61      0.63      4392\n",
      "weighted avg       0.71      0.72      0.71      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: LogisticRegression(max_iter=300) - TARGET SPACE: original\n",
      "\t === Model training: 0.7300000190734863 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      2753\n",
      "           1       0.67      0.50      0.57       930\n",
      "           2       0.78      0.55      0.65       709\n",
      "\n",
      "    accuracy                           0.78      4392\n",
      "   macro avg       0.75      0.66      0.70      4392\n",
      "weighted avg       0.77      0.78      0.77      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: DecisionTreeClassifier() - TARGET SPACE: original\n",
      "\t === Model training: 1.126000165939331 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78      2753\n",
      "           1       0.38      0.35      0.37       930\n",
      "           2       0.55      0.53      0.54       709\n",
      "\n",
      "    accuracy                           0.66      4392\n",
      "   macro avg       0.57      0.56      0.56      4392\n",
      "weighted avg       0.65      0.66      0.66      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: RandomForestClassifier() - TARGET SPACE: original\n",
      "\t === Model training: 7.042999982833862 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85      2753\n",
      "           1       0.69      0.37      0.48       930\n",
      "           2       0.73      0.50      0.60       709\n",
      "\n",
      "    accuracy                           0.76      4392\n",
      "   macro avg       0.73      0.61      0.64      4392\n",
      "weighted avg       0.75      0.76      0.73      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: SVC(C=1) - TARGET SPACE: original\n",
      "\t === Model training: 8.375 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.86      2753\n",
      "           1       0.71      0.42      0.53       930\n",
      "           2       0.81      0.56      0.66       709\n",
      "\n",
      "    accuracy                           0.78      4392\n",
      "   macro avg       0.77      0.65      0.68      4392\n",
      "weighted avg       0.77      0.78      0.76      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42) - TARGET SPACE: original\n",
      "\t === Model training: 226.01550006866455 sec\n",
      "\t ========= Classification Report\n",
      "\t               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      2753\n",
      "           1       0.52      0.51      0.52       930\n",
      "           2       0.65      0.59      0.62       709\n",
      "\n",
      "    accuracy                           0.74      4392\n",
      "   macro avg       0.67      0.65      0.66      4392\n",
      "weighted avg       0.73      0.74      0.74      4392\n",
      "\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for clf in models:\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"=================================================================================================\")\n",
    "    print(\"\\t === CLASSIFIER:\", clf, \"- TARGET SPACE: original\")\n",
    "    print(\"\\t === Model training:\", t1 - t0, \"sec\")\n",
    "\n",
    "    y_predicted = clf.predict(X_test_vec)\n",
    "\n",
    "    print(\"\\t ========= Classification Report\")\n",
    "    print(\"\\t\", classification_report(y_test, y_predicted))\n",
    "    print(\"=================================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806daa5",
   "metadata": {},
   "source": [
    "# Models on the reduced dimensional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c025be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Working with target space of 10 dimensions ============\n",
      "\tInput space dimensionality (training): (10248, 10) (10248, 9849)\n",
      "\tInput space dimensionality (testing): (4392, 10) (4392, 9849)\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: KNeighborsClassifier(n_neighbors=10) - TARGET SPACE: 10\n",
      "\t === Model training: 0.0409998893737793 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83      2753\n",
      "           1       0.52      0.32      0.39       930\n",
      "           2       0.62      0.42      0.50       709\n",
      "\n",
      "    accuracy                           0.71      4392\n",
      "   macro avg       0.63      0.55      0.57      4392\n",
      "weighted avg       0.68      0.71      0.68      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: LogisticRegression(max_iter=300) - TARGET SPACE: 10\n",
      "\t === Model training: 0.1770002841949463 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.96      0.82      2753\n",
      "           1       0.55      0.16      0.25       930\n",
      "           2       0.67      0.37      0.48       709\n",
      "\n",
      "    accuracy                           0.70      4392\n",
      "   macro avg       0.64      0.50      0.52      4392\n",
      "weighted avg       0.67      0.70      0.64      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: DecisionTreeClassifier() - TARGET SPACE: 10\n",
      "\t === Model training: 0.1249997615814209 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      2753\n",
      "           1       0.36      0.36      0.36       930\n",
      "           2       0.44      0.44      0.44       709\n",
      "\n",
      "    accuracy                           0.63      4392\n",
      "   macro avg       0.52      0.52      0.52      4392\n",
      "weighted avg       0.63      0.63      0.63      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: RandomForestClassifier() - TARGET SPACE: 10\n",
      "\t === Model training: 2.4519999027252197 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      2753\n",
      "           1       0.52      0.30      0.38       930\n",
      "           2       0.62      0.48      0.54       709\n",
      "\n",
      "    accuracy                           0.71      4392\n",
      "   macro avg       0.63      0.56      0.58      4392\n",
      "weighted avg       0.68      0.71      0.69      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: SVC(C=1) - TARGET SPACE: 10\n",
      "\t === Model training: 3.732999801635742 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82      2753\n",
      "           1       0.56      0.26      0.35       930\n",
      "           2       0.69      0.39      0.50       709\n",
      "\n",
      "    accuracy                           0.71      4392\n",
      "   macro avg       0.66      0.53      0.56      4392\n",
      "weighted avg       0.69      0.71      0.67      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42) - TARGET SPACE: 10\n",
      "\t === Model training: 8.598999977111816 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.84      2753\n",
      "           1       0.59      0.33      0.42       930\n",
      "           2       0.69      0.44      0.54       709\n",
      "\n",
      "    accuracy                           0.73      4392\n",
      "   macro avg       0.68      0.57      0.60      4392\n",
      "weighted avg       0.71      0.73      0.70      4392\n",
      "\n",
      "=================================================================================================\n",
      "========== Working with target space of 100 dimensions ============\n",
      "\tInput space dimensionality (training): (10248, 100) (10248, 9849)\n",
      "\tInput space dimensionality (testing): (4392, 100) (4392, 9849)\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: KNeighborsClassifier(n_neighbors=10) - TARGET SPACE: 100\n",
      "\t === Model training: 0.002000093460083008 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82      2753\n",
      "           1       0.48      0.38      0.43       930\n",
      "           2       0.66      0.42      0.51       709\n",
      "\n",
      "    accuracy                           0.70      4392\n",
      "   macro avg       0.63      0.56      0.58      4392\n",
      "weighted avg       0.68      0.70      0.68      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: LogisticRegression(max_iter=300) - TARGET SPACE: 100\n",
      "\t === Model training: 0.3100001811981201 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.84      2753\n",
      "           1       0.61      0.37      0.46       930\n",
      "           2       0.75      0.49      0.59       709\n",
      "\n",
      "    accuracy                           0.75      4392\n",
      "   macro avg       0.71      0.60      0.63      4392\n",
      "weighted avg       0.73      0.75      0.72      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: DecisionTreeClassifier() - TARGET SPACE: 100\n",
      "\t === Model training: 1.3500001430511475 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75      2753\n",
      "           1       0.37      0.37      0.37       930\n",
      "           2       0.42      0.44      0.43       709\n",
      "\n",
      "    accuracy                           0.62      4392\n",
      "   macro avg       0.52      0.52      0.52      4392\n",
      "weighted avg       0.62      0.62      0.62      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: RandomForestClassifier() - TARGET SPACE: 100\n",
      "\t === Model training: 8.27400016784668 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84      2753\n",
      "           1       0.65      0.29      0.40       930\n",
      "           2       0.72      0.44      0.55       709\n",
      "\n",
      "    accuracy                           0.73      4392\n",
      "   macro avg       0.70      0.56      0.60      4392\n",
      "weighted avg       0.72      0.73      0.70      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: SVC(C=1) - TARGET SPACE: 100\n",
      "\t === Model training: 7.038000106811523 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      2753\n",
      "           1       0.65      0.37      0.47       930\n",
      "           2       0.74      0.52      0.61       709\n",
      "\n",
      "    accuracy                           0.75      4392\n",
      "   macro avg       0.72      0.61      0.64      4392\n",
      "weighted avg       0.74      0.75      0.73      4392\n",
      "\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "\t === CLASSIFIER: MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42) - TARGET SPACE: 100\n",
      "\t === Model training: 12.201000213623047 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      2753\n",
      "           1       0.52      0.48      0.50       930\n",
      "           2       0.61      0.56      0.58       709\n",
      "\n",
      "    accuracy                           0.72      4392\n",
      "   macro avg       0.64      0.63      0.63      4392\n",
      "weighted avg       0.71      0.72      0.71      4392\n",
      "\n",
      "=================================================================================================\n",
      "========== Working with target space of 1000 dimensions ============\n",
      "\tInput space dimensionality (training): (10248, 1000) (10248, 9849)\n",
      "\tInput space dimensionality (testing): (4392, 1000) (4392, 9849)\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: KNeighborsClassifier(n_neighbors=10) - TARGET SPACE: 1000\n",
      "\t === Model training: 0.012999773025512695 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.57      0.65      2753\n",
      "           1       0.27      0.59      0.37       930\n",
      "           2       0.74      0.28      0.40       709\n",
      "\n",
      "    accuracy                           0.53      4392\n",
      "   macro avg       0.59      0.48      0.47      4392\n",
      "weighted avg       0.65      0.53      0.55      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: LogisticRegression(max_iter=300) - TARGET SPACE: 1000\n",
      "\t === Model training: 1.3329999446868896 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.86      2753\n",
      "           1       0.66      0.48      0.56       930\n",
      "           2       0.79      0.55      0.65       709\n",
      "\n",
      "    accuracy                           0.78      4392\n",
      "   macro avg       0.75      0.66      0.69      4392\n",
      "weighted avg       0.77      0.78      0.76      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: DecisionTreeClassifier() - TARGET SPACE: 1000\n",
      "\t === Model training: 17.488999843597412 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      2753\n",
      "           1       0.35      0.34      0.34       930\n",
      "           2       0.43      0.46      0.44       709\n",
      "\n",
      "    accuracy                           0.62      4392\n",
      "   macro avg       0.51      0.52      0.52      4392\n",
      "weighted avg       0.62      0.62      0.62      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: RandomForestClassifier() - TARGET SPACE: 1000\n",
      "\t === Model training: 29.516000032424927 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.99      0.81      2753\n",
      "           1       0.76      0.17      0.28       930\n",
      "           2       0.82      0.25      0.38       709\n",
      "\n",
      "    accuracy                           0.70      4392\n",
      "   macro avg       0.75      0.47      0.49      4392\n",
      "weighted avg       0.72      0.70      0.63      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: SVC(C=1) - TARGET SPACE: 1000\n",
      "\t === Model training: 40.758999824523926 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87      2753\n",
      "           1       0.70      0.44      0.54       930\n",
      "           2       0.78      0.58      0.67       709\n",
      "\n",
      "    accuracy                           0.78      4392\n",
      "   macro avg       0.76      0.66      0.69      4392\n",
      "weighted avg       0.78      0.78      0.77      4392\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "\t === CLASSIFIER: MLPClassifier(hidden_layer_sizes=(128, 16), random_state=42) - TARGET SPACE: 1000\n",
      "\t === Model training: 38.47800016403198 sec\n",
      "\t === Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85      2753\n",
      "           1       0.57      0.48      0.52       930\n",
      "           2       0.67      0.65      0.66       709\n",
      "\n",
      "    accuracy                           0.75      4392\n",
      "   macro avg       0.69      0.67      0.68      4392\n",
      "weighted avg       0.75      0.75      0.75      4392\n",
      "\n",
      "=================================================================================================\n",
      "========== Working with target space of 10000 dimensions ============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components must be < n_features; got 10000 >= 9849",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-98f7967ea530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"========== Working with target space of\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dimensions ============\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mSVD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX_train_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX_test_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 raise ValueError(\"n_components must be < n_features;\"\n\u001b[0m\u001b[0;32m    181\u001b[0m                                  \" got %d >= %d\" % (k, n_features))\n\u001b[0;32m    182\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n",
      "\u001b[1;31mValueError\u001b[0m: n_components must be < n_features; got 10000 >= 9849"
     ]
    }
   ],
   "source": [
    "reduced_spaces = [ 10, 100, 1000, 10000 ]\n",
    "for target_space in reduced_spaces:\n",
    "    print(\"========== Working with target space of\", target_space, \"dimensions ============\")\n",
    "    SVD = TruncatedSVD(n_components=target_space, random_state=42)\n",
    "    X_train_red = SVD.fit_transform(X_train_vec)\n",
    "    X_test_red = SVD.transform(X_test_vec)\n",
    "\n",
    "    print(\"\\tInput space dimensionality (training):\", X_train_red.shape, X_train_vec.shape)\n",
    "    print(\"\\tInput space dimensionality (testing):\", X_test_red.shape, X_test_vec.shape)\n",
    "\n",
    "    for clf in models:\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train_red, y_train)\n",
    "        t1 = time.time()\n",
    "        print(\"=================================================================================================\")\n",
    "        print(\"\\t === CLASSIFIER:\", clf, \"- TARGET SPACE:\", target_space)\n",
    "        print(\"\\t === Model training:\", t1 - t0, \"sec\")\n",
    "\n",
    "        y_predicted = clf.predict(X_test_red)\n",
    "\n",
    "        print(\"\\t === Classification Report\")\n",
    "        print(classification_report(y_test, y_predicted))\n",
    "        print(\"=================================================================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
